{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LpLoss(object):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
    "        super(LpLoss, self).__init__()\n",
    "\n",
    "        #Dimension and Lp-norm type are postive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def abs(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        #Assume uniform mesh\n",
    "        h = 1.0 / (x.size()[1] - 1.0)\n",
    "\n",
    "        all_norms = (h**(self.d/self.p))*torch.norm(x.view(num_examples,-1) - y.view(num_examples,-1), self.p, 1)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(all_norms)\n",
    "            else:\n",
    "                return torch.sum(all_norms)\n",
    "\n",
    "        return all_norms\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        diff_norms = torch.norm(x.reshape(num_examples,-1) - y.reshape(num_examples,-1), self.p, 1)\n",
    "        y_norms = torch.norm(y.reshape(num_examples,-1), self.p, 1)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(diff_norms/y_norms)\n",
    "            else:\n",
    "                return torch.sum(diff_norms/y_norms)\n",
    "\n",
    "        return diff_norms/y_norms\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.rel(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(torch.nn.Module):\n",
    "    '''Standard module format. \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Module, self).__init__()\n",
    "        self.activation = None\n",
    "        self.initializer = None\n",
    "        \n",
    "        self.__device = None\n",
    "        self.__dtype = None\n",
    "        \n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.__device\n",
    "        \n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.__dtype\n",
    "\n",
    "    @device.setter\n",
    "    def device(self, d):\n",
    "        if d == 'cpu':\n",
    "            self.cpu()\n",
    "        elif d == 'gpu':\n",
    "            self.cuda()\n",
    "        else:\n",
    "            raise ValueError\n",
    "        self.__device = d\n",
    "    \n",
    "    @dtype.setter    \n",
    "    def dtype(self, d):\n",
    "        if d == 'float':\n",
    "            self.to(torch.float)\n",
    "        elif d == 'double':\n",
    "            self.to(torch.double)\n",
    "        else:\n",
    "            raise ValueError\n",
    "        self.__dtype = d\n",
    "\n",
    "    @property\n",
    "    def Device(self):\n",
    "        if self.__device == 'cpu':\n",
    "            return torch.device('cpu')\n",
    "        elif self.__device == 'gpu':\n",
    "            return torch.device('cuda')\n",
    "        \n",
    "    @property\n",
    "    def Dtype(self):\n",
    "        if self.__dtype == 'float':\n",
    "            return torch.float32\n",
    "        elif self.__dtype == 'double':\n",
    "            return torch.float64\n",
    "\n",
    "    @property\n",
    "    def act(self):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return torch.sigmoid\n",
    "        elif self.activation == 'relu':\n",
    "            return torch.relu\n",
    "        elif self.activation == 'tanh':\n",
    "            return torch.tanh\n",
    "        elif self.activation == 'elu':\n",
    "            return torch.elu\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    \n",
    "    @property        \n",
    "    def Act(self):\n",
    "        if self.activation == 'sigmoid':\n",
    "            return torch.nn.Sigmoid()\n",
    "        elif self.activation == 'relu':\n",
    "            return torch.nn.ReLU()\n",
    "        elif self.activation == 'tanh':\n",
    "            return torch.nn.Tanh()\n",
    "        elif self.activation == 'elu':\n",
    "            return torch.nn.ELU()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def weight_init_(self):\n",
    "        if self.initializer == 'He normal':\n",
    "            return torch.nn.init.kaiming_normal_\n",
    "        elif self.initializer == 'He uniform':\n",
    "            return torch.nn.init.kaiming_uniform_\n",
    "        elif self.initializer == 'Glorot normal':\n",
    "            return torch.nn.init.xavier_normal_\n",
    "        elif self.initializer == 'Glorot uniform':\n",
    "            return torch.nn.init.xavier_uniform_\n",
    "        elif self.initializer == 'orthogonal':\n",
    "            return torch.nn.init.orthogonal_\n",
    "        elif self.initializer == 'default':\n",
    "            if self.activation == 'relu':\n",
    "                return torch.nn.init.kaiming_normal_\n",
    "            elif self.activation == 'tanh':\n",
    "                return torch.nn.init.orthogonal_\n",
    "            else:\n",
    "                return lambda x: None\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructureNN(Module):\n",
    "    '''Structure-oriented neural network used as a general map based on designing architecture.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(StructureNN, self).__init__()\n",
    "        \n",
    "    def predict(self, x, returnnp=False):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=self.Dtype, device=self.Device)\n",
    "        return self(x).cpu().detach().numpy() if returnnp else self(x)\n",
    "\n",
    "\n",
    "class FNN(StructureNN):\n",
    "    '''Fully connected neural networks.\n",
    "    '''\n",
    "    def __init__(self, ind, outd, layers=2, width=50, activation='relu', initializer='default', softmax=False):\n",
    "        super(FNN, self).__init__()\n",
    "        self.ind = ind\n",
    "        self.outd = outd\n",
    "        self.layers = layers\n",
    "        self.width = width\n",
    "        self.activation = activation\n",
    "        self.softmax = softmax\n",
    "        \n",
    "        self.modus = self.__init_modules()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i in range(1, self.layers):\n",
    "            LinM = self.modus['LinM{}'.format(i)]\n",
    "            NonM = self.modus['NonM{}'.format(i)]\n",
    "            x = NonM(LinM(x))\n",
    "        x = self.modus['LinMout'](x)\n",
    "        if self.softmax:\n",
    "            x = nn.functional.softmax(x, dim=-1)\n",
    "        return x\n",
    "    \n",
    "    def __init_modules(self):\n",
    "        modules = nn.ModuleDict()\n",
    "        if self.layers > 1:\n",
    "            modules['LinM1'] = nn.Linear(self.ind, self.width)\n",
    "            modules['NonM1'] = self.Act\n",
    "            for i in range(2, self.layers):\n",
    "                modules['LinM{}'.format(i)] = nn.Linear(self.width, self.width)\n",
    "                modules['NonM{}'.format(i)] = self.Act\n",
    "            modules['LinMout'] = nn.Linear(self.width, self.outd)\n",
    "        else:\n",
    "            modules['LinMout'] = nn.Linear(self.ind, self.outd)\n",
    "            \n",
    "        return modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepONet(StructureNN):\n",
    "    '''Deep operator network.\n",
    "    Input: [batch size, branch_dim + trunk_dim]\n",
    "    Output: [batch size, 1]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, branch_dim, trunk_dim, branch_depth=2, trunk_depth=3, width=50,\n",
    "                 activation='relu'):\n",
    "        super(DeepONet, self).__init__()\n",
    "        self.branch_dim = branch_dim\n",
    "        self.trunk_dim = trunk_dim\n",
    "        self.branch_depth = branch_depth\n",
    "        self.trunk_depth = trunk_depth\n",
    "        self.width = width\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.modus = self.__init_modules()\n",
    "        self.params = self.__init_params()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_branch, x_trunk = x[..., :self.branch_dim], x[..., -self.trunk_dim:]\n",
    "        # x_branch = self.modus['Branch'](x_branch)\n",
    "        for i in range(1, self.branch_depth):\n",
    "            x_branch = self.modus['BrActM{}'.format(i)](self.modus['BrLinM{}'.format(i)](x_branch))\n",
    "        x_branch = self.modus['BrLinM{}'.format(self.branch_depth)](x_branch)\n",
    "\n",
    "        for i in range(1, self.trunk_depth):\n",
    "            x_trunk = self.modus['TrActM{}'.format(i)](self.modus['TrLinM{}'.format(i)](x_trunk))\n",
    "        return torch.sum(x_branch * x_trunk, dim=-1, keepdim=True) + self.params['bias']\n",
    "\n",
    "    def trunk_forward(self, x):\n",
    "        x_trunk = x[..., -self.trunk_dim:]\n",
    "        \n",
    "        for i in range(1, self.trunk_depth):\n",
    "            x_trunk = self.modus['TrActM{}'.format(i)](self.modus['TrLinM{}'.format(i)](x_trunk))\n",
    "        return x_trunk\n",
    "    \n",
    "    def branch_forward(self, x):\n",
    "        x_branch = x[..., :self.branch_dim]\n",
    "        \n",
    "        for i in range(1, self.branch_depth):\n",
    "            x_branch = self.modus['BrActM{}'.format(i)](self.modus['BrLinM{}'.format(i)](x_branch))\n",
    "        x_branch = self.modus['BrLinM{}'.format(self.branch_depth)](x_branch)\n",
    "        \n",
    "        return x_branch\n",
    "        \n",
    "    def __init_modules(self):\n",
    "        modules = nn.ModuleDict()\n",
    "        # modules['Branch'] = FNN(self.branch_dim, self.width, self.branch_depth, self.width,\n",
    "        #                         self.activation, self.initializer)\n",
    "        if self.branch_depth > 1:\n",
    "            modules['BrLinM1'] = nn.Linear(self.branch_dim, self.width)\n",
    "            modules['BrActM1'] = self.Act\n",
    "            for i in range(2, self.branch_depth):\n",
    "                modules['BrLinM{}'.format(i)] = nn.Linear(self.width, self.width)\n",
    "                modules['BrActM{}'.format(i)] = self.Act\n",
    "            modules['BrLinM{}'.format(self.branch_depth)] = nn.Linear(self.width, self.width)\n",
    "        else:\n",
    "            modules['BrLinM{}'.format(self.branch_depth)] = nn.Linear(self.branch_dim, self.width)\n",
    "            \n",
    "\n",
    "        modules['TrLinM1'] = nn.Linear(self.trunk_dim, self.width)\n",
    "        modules['TrActM1'] = self.Act\n",
    "        for i in range(2, self.trunk_depth):\n",
    "            modules['TrLinM{}'.format(i)] = nn.Linear(self.width, self.width)\n",
    "            modules['TrActM{}'.format(i)] = self.Act\n",
    "        return modules\n",
    "\n",
    "            \n",
    "    def __init_params(self):\n",
    "        params = nn.ParameterDict()\n",
    "        params['bias'] = nn.Parameter(torch.zeros([1]))\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFFONet(StructureNN):\n",
    "    '''Deep full field operator network.\n",
    "    Input: [batch size, branch_dim + trunk_dim * Np]\n",
    "    Output: [batch size, 1]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, branch_dim, trunk_dim, x_trunk, branch_depth=2, trunk_depth=3, width=50,\n",
    "                 activation='relu'):\n",
    "        super(DeepFFONet, self).__init__()\n",
    "        self.branch_dim = branch_dim\n",
    "        self.trunk_dim = trunk_dim\n",
    "        self.x_trunk = x_trunk\n",
    "        self.branch_depth = branch_depth\n",
    "        self.trunk_depth = trunk_depth\n",
    "        self.width = width\n",
    "        self.activation = activation\n",
    "        \n",
    "        self.modus = self.__init_modules()\n",
    "        self.params = self.__init_params()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_branch, x_trunk = x[..., :self.branch_dim], self.x_trunk\n",
    "        # x_branch = self.modus['Branch'](x_branch)\n",
    "        for i in range(1, self.branch_depth):\n",
    "            x_branch = self.modus['BrActM{}'.format(i)](self.modus['BrLinM{}'.format(i)](x_branch))\n",
    "        x_branch = self.modus['BrLinM{}'.format(self.branch_depth)](x_branch)\n",
    "\n",
    "        for i in range(1, self.trunk_depth):\n",
    "            x_trunk = self.modus['TrActM{}'.format(i)](self.modus['TrLinM{}'.format(i)](x_trunk))\n",
    "        \n",
    "        \n",
    "        return torch.matmul(x_branch , x_trunk.T) + self.params['bias']\n",
    "\n",
    "    def trunk_forward(self):\n",
    "        x_trunk = self.x_trunk\n",
    "        \n",
    "        for i in range(1, self.trunk_depth):\n",
    "            x_trunk = self.modus['TrActM{}'.format(i)](self.modus['TrLinM{}'.format(i)](x_trunk))\n",
    "        return x_trunk\n",
    "    \n",
    "    def branch_forward(self, x):\n",
    "        x_branch = x[..., :self.branch_dim]\n",
    "        \n",
    "        for i in range(1, self.branch_depth):\n",
    "            x_branch = self.modus['BrActM{}'.format(i)](self.modus['BrLinM{}'.format(i)](x_branch))\n",
    "        x_branch = self.modus['BrLinM{}'.format(self.branch_depth)](x_branch)\n",
    "        \n",
    "        return x_branch\n",
    "        \n",
    "    def __init_modules(self):\n",
    "        modules = nn.ModuleDict()\n",
    "        # modules['Branch'] = FNN(self.branch_dim, self.width, self.branch_depth, self.width,\n",
    "        #                         self.activation, self.initializer)\n",
    "        if self.branch_depth > 1:\n",
    "            modules['BrLinM1'] = nn.Linear(self.branch_dim, self.width)\n",
    "            modules['BrActM1'] = self.Act\n",
    "            for i in range(2, self.branch_depth):\n",
    "                modules['BrLinM{}'.format(i)] = nn.Linear(self.width, self.width)\n",
    "                modules['BrActM{}'.format(i)] = self.Act\n",
    "            modules['BrLinM{}'.format(self.branch_depth)] = nn.Linear(self.width, self.width)\n",
    "        else:\n",
    "            modules['BrLinM{}'.format(self.branch_depth)] = nn.Linear(self.branch_dim, self.width)\n",
    "            \n",
    "\n",
    "        modules['TrLinM1'] = nn.Linear(self.trunk_dim, self.width)\n",
    "        modules['TrActM1'] = self.Act\n",
    "        for i in range(2, self.trunk_depth):\n",
    "            modules['TrLinM{}'.format(i)] = nn.Linear(self.width, self.width)\n",
    "            modules['TrActM{}'.format(i)] = self.Act\n",
    "        return modules\n",
    "\n",
    "            \n",
    "    def __init_params(self):\n",
    "        params = nn.ParameterDict()\n",
    "        params['bias'] = nn.Parameter(torch.zeros([1]))\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCAONet(StructureNN):\n",
    "    '''PCA operator network.\n",
    "    Input:  [batch size, branch_dim]\n",
    "    Output: [batch size, field_size]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, ind, outd, bases, layers=2, width=50, activation='relu', initializer='default'):\n",
    "        super(PCAONet, self).__init__()\n",
    "        self.ind = ind\n",
    "        self.outd = outd\n",
    "        self.layers = layers\n",
    "        self.width = width\n",
    "        self.activation = activation\n",
    "        self.bases = bases\n",
    "        assert(bases.shape[0] == outd)\n",
    "        \n",
    "        self.modus = self.__init_modules()\n",
    "        self.params = self.__init_params()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        for i in range(1, self.layers):\n",
    "            LinM = self.modus['LinM{}'.format(i)]\n",
    "            NonM = self.modus['NonM{}'.format(i)]\n",
    "            x = NonM(LinM(x))\n",
    "        x = self.modus['LinMout'](x)\n",
    "        x = torch.matmul(x , self.bases) + self.params['bias']\n",
    "        \n",
    "       \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def __init_modules(self):\n",
    "        modules = nn.ModuleDict()\n",
    "        if self.layers > 1:\n",
    "            modules['LinM1'] = nn.Linear(self.ind, self.width)\n",
    "            modules['NonM1'] = self.Act\n",
    "            for i in range(2, self.layers):\n",
    "                modules['LinM{}'.format(i)] = nn.Linear(self.width, self.width)\n",
    "                modules['NonM{}'.format(i)] = self.Act\n",
    "            modules['LinMout'] = nn.Linear(self.width, self.outd)\n",
    "        else:\n",
    "            modules['LinMout'] = nn.Linear(self.ind, self.outd)\n",
    "            \n",
    "        return modules\n",
    "        \n",
    "\n",
    "            \n",
    "    def __init_params(self):\n",
    "        params = nn.ParameterDict()\n",
    "        params['bias'] = nn.Parameter(torch.zeros([1]))\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################\n",
    "# fourier neural operator\n",
    "################################################################\n",
    "class SpectralConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1, modes2):\n",
    "        super(SpectralConv2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        2D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1 #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "        self.modes2 = modes2\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, self.modes2, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul2d(self, input, weights):\n",
    "        # (batch, in_channel, x,y ), (in_channel, out_channel, x,y) -> (batch, out_channel, x,y)\n",
    "        return torch.einsum(\"bixy,ioxy->boxy\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft2(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)\n",
    "        out_ft[:, :, :self.modes1, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)\n",
    "        out_ft[:, :, -self.modes1:, :self.modes2] = \\\n",
    "            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))\n",
    "        return x\n",
    "\n",
    "class FNO2d(nn.Module):\n",
    "    def __init__(self, modes1, modes2,  width):\n",
    "        super(FNO2d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the coefficient function and locations (a(x, y), x, y)\n",
    "        input shape: (batchsize, x=s, y=s, c=3)\n",
    "        output: the solution \n",
    "        output shape: (batchsize, x=s, y=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes1\n",
    "        self.modes2 = modes2\n",
    "        self.width = width\n",
    "        self.padding = 9 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(1, self.width) # input channel is 3: (a(x, y), x, y)\n",
    "\n",
    "        self.conv0 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv1 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "        self.conv2 = SpectralConv2d(self.width, self.width, self.modes1, self.modes2)\n",
    "\n",
    "        self.w0 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv2d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv2d(self.width, self.width, 1)\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 1)\n",
    "        # self.fc1 = nn.Linear(self.width, 128)\n",
    "        # self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # grid = self.get_grid(x.shape, x.device)\n",
    "        # x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.pad(x, [0,self.padding, 0,self.padding])\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        # if self.padding > 0:\n",
    "        x = x[..., :-self.padding, :-self.padding]\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.fc1(x)\n",
    "        # x = F.gelu(x)\n",
    "        # x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    # def get_grid(self, shape, device):\n",
    "    #     batchsize, size_x, size_y = shape[0], shape[1], shape[2]\n",
    "    #     gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "    #     gridx = gridx.reshape(1, size_x, 1, 1).repeat([batchsize, 1, size_y, 1])\n",
    "    #     gridy = torch.tensor(np.linspace(0, 1, size_y), dtype=torch.float)\n",
    "    #     gridy = gridy.reshape(1, 1, size_y, 1).repeat([batchsize, size_x, 1, 1])\n",
    "    #     return torch.cat((gridx, gridy), dim=-1).to(device)\n",
    "\n",
    "################################################################\n",
    "# configs\n",
    "################################################################\n",
    "# TRAIN_PATH = 'data/piececonst_r421_N1024_smooth1.mat'\n",
    "# TEST_PATH = 'data/piececonst_r421_N1024_smooth2.mat'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################################################################\n",
    "#  1d fourier layer\n",
    "################################################################\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes1):\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes1  #Number of Fourier modes to multiply, at most floor(N/2) + 1\n",
    "\n",
    "        self.scale = (1 / (in_channels*out_channels))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, input, weights):\n",
    "        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n",
    "        return torch.einsum(\"bix,iox->box\", input, weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.shape[0]\n",
    "        #Compute Fourier coeffcients up to factor of e^(- something constant)\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "\n",
    "        # Multiply relevant Fourier modes\n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1)//2 + 1,  device=x.device, dtype=torch.cfloat)\n",
    "        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n",
    "\n",
    "        #Return to physical space\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return x\n",
    "\n",
    "class FNO1d(nn.Module):\n",
    "    def __init__(self, modes, width):\n",
    "        super(FNO1d, self).__init__()\n",
    "\n",
    "        \"\"\"\n",
    "        The overall network. It contains 4 layers of the Fourier layer.\n",
    "        1. Lift the input to the desire channel dimension by self.fc0 .\n",
    "        2. 4 layers of the integral operators u' = (W + K)(u).\n",
    "            W defined by self.w; K defined by self.conv .\n",
    "        3. Project from the channel space to the output space by self.fc1 and self.fc2 .\n",
    "        \n",
    "        input: the solution of the initial condition and location (a(x), x)\n",
    "        input shape: (batchsize, x=s, c=2)\n",
    "        output: the solution of a later timestep\n",
    "        output shape: (batchsize, x=s, c=1)\n",
    "        \"\"\"\n",
    "\n",
    "        self.modes1 = modes\n",
    "        self.width = width\n",
    "        self.padding = 2 # pad the domain if input is non-periodic\n",
    "        self.fc0 = nn.Linear(1, self.width) # input channel is 2: (a(x), x)\n",
    "\n",
    "        self.conv0 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.conv1 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        self.conv2 = SpectralConv1d(self.width, self.width, self.modes1)\n",
    "        \n",
    "        self.w0 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w1 = nn.Conv1d(self.width, self.width, 1)\n",
    "        self.w2 = nn.Conv1d(self.width, self.width, 1)\n",
    "        \n",
    "\n",
    "        self.fc1 = nn.Linear(self.width, 1)\n",
    "      \n",
    "\n",
    "    def forward(self, x):\n",
    "#         grid = self.get_grid(x.shape, x.device)\n",
    "#         x = torch.cat((x, grid), dim=-1)\n",
    "        x = self.fc0(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # x = F.pad(x, [0,self.padding]) # pad the domain if input is non-periodic\n",
    "\n",
    "        x1 = self.conv0(x)\n",
    "        x2 = self.w0(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.w1(x)\n",
    "        x = x1 + x2\n",
    "        x = F.gelu(x)\n",
    "\n",
    "        x1 = self.conv2(x)\n",
    "        x2 = self.w2(x)\n",
    "        x = x1 + x2\n",
    "\n",
    "        # x = x[..., :-self.padding] # pad the domain if input is non-periodic\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "    def get_grid(self, shape, device):\n",
    "        batchsize, size_x = shape[0], shape[1]\n",
    "        gridx = torch.tensor(np.linspace(0, 1, size_x), dtype=torch.float)\n",
    "        gridx = gridx.reshape(1, size_x, 1).repeat([batchsize, 1, 1])\n",
    "        return gridx.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print the number of parameters\n",
    "# def count_params(model):\n",
    "#     c = 0\n",
    "#     for p in list(model.parameters()):\n",
    "#         c += reduce(operator.mul, list(p.size()))\n",
    "#     return c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
