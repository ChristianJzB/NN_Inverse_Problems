{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyDOE import lhs\n",
    "from scipy.stats import uniform,norm\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "os.chdir(\"/home/s2113174/Projects-1\")\n",
    "\n",
    "#np.random.seed(1234)\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        # Number of layers\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # Activation Function\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        # The following loop organized the layers of the NN         \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1])))\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # Deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "        # for param in self.parameters():\n",
    "        #     if len(param.shape) > 1:\n",
    "        #         torch.nn.init.xavier_normal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set(max_space = 2,obs = 1,param = 1 ,mean = 0,std = 0):\n",
    "\n",
    "    t= np.linspace(0,max_space,obs)\n",
    "\n",
    "    sol = (param/ (2*np.pi))*np.sin(2*np.pi*t)\n",
    "\n",
    "    noise_sol_test = sol + np.random.normal(mean,std, len(t))\n",
    "\n",
    "    return t,noise_sol_test\n",
    "\n",
    "\n",
    "def data(max_space = 2,obs = 1,param = 1 ,mean = 0,std = 0.1):\n",
    "\n",
    "    t= np.linspace(0,max_space,obs)\n",
    "\n",
    "    sol = (param/ (2*np.pi))*np.sin(2*np.pi*t)\n",
    "\n",
    "    noise_sol_test = sol + np.random.normal(mean,std, len(t))\n",
    "\n",
    "    x,y = torch.tensor(t).float().reshape(-1,1),torch.tensor(noise_sol_test).float().reshape(-1,1)\n",
    "    \n",
    "    X_u_train = TensorDataset(x,y)\n",
    "\n",
    "    X_u_train = DataLoader(X_u_train,batch_size=obs)\n",
    "\n",
    "    return X_u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y = test_set(obs = 1)\n",
    "\n",
    "layers = [1] + 1*[10] + [1]\n",
    "model = DNN(layers)\n",
    "loss = torch.nn.MSELoss(reduction ='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.layer_0.weight\n",
      ".diag_ggn_exact.shape:    tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "layers.layer_0.bias\n",
      ".diag_ggn_exact.shape:    tensor([0.0398, 0.0431, 0.0430, 0.0165, 0.0780, 0.0144, 0.0238, 0.1425, 0.0424,\n",
      "        0.1130])\n",
      "layers.layer_1.weight\n",
      ".diag_ggn_exact.shape:    tensor([[0.9492, 0.4634, 0.2001, 0.3443, 0.6250, 0.5839, 0.1906, 0.2988, 0.1662,\n",
      "         0.0585]])\n",
      "layers.layer_1.bias\n",
      ".diag_ggn_exact.shape:    tensor([2.0000])\n"
     ]
    }
   ],
   "source": [
    "from backpack import backpack, extend\n",
    "from backpack.extensions import DiagHessian, DiagGGNExact\n",
    "\n",
    "model_ = extend(model, use_converter=True)\n",
    "lossfunc_ = extend(loss)\n",
    "\n",
    "loss_ = lossfunc_(model_(Variable(torch.tensor(t).float().reshape(-1,1),requires_grad=True)), torch.tensor(y).float().reshape(-1,1))\n",
    "\n",
    "with backpack(DiagHessian(), DiagGGNExact()):\n",
    "    loss_.backward()\n",
    "\n",
    "for name, param in model_.named_parameters():\n",
    "    print(name)\n",
    "    print(\".diag_ggn_exact.shape:   \", param.diag_ggn_exact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4746, 0.2317, 0.1001, 0.1722, 0.3125, 0.2920, 0.0953, 0.1494, 0.0831,\n",
       "        0.0293, 1.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from laplace import Laplace\n",
    "\n",
    "la = Laplace(model, 'regression', subset_of_weights='last_layer', hessian_structure='diag')\n",
    "\n",
    "dta = data(obs = 1)\n",
    "\n",
    "la.fit(dta)\n",
    "\n",
    "la.H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.4833]]), tensor([[[2.0340]]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "layers\n",
      "layers.layer_0\n",
      "layers.activation_0\n",
      "layers.layer_1\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict to store the activations\n",
    "forw_activation,back_activation = {},{}\n",
    "def forw_getActivation(name):\n",
    "  # the hook signature\n",
    "  def hook(model, input, output):\n",
    "    forw_activation[name] = output.detach()\n",
    "  return hook\n",
    "\n",
    "def back_getActivation(name):\n",
    "  # the hook signature\n",
    "  def hook(model, input, output):\n",
    "    back_activation[name] = output[0].detach()\n",
    "  return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = model.layers[1].register_forward_hook(forw_getActivation('layers.activation_0'))\n",
    "h2 = model.layers[2].register_forward_hook(forw_getActivation('layers.layer_1'))\n",
    "h3 = model.layers[0].register_forward_hook(forw_getActivation('layers.layer_0'))\n",
    "\n",
    "b_h1 = model.layers[1].register_full_backward_hook(back_getActivation('layers.activation_0'))\n",
    "b_h2 = model.layers[2].register_full_backward_hook(back_getActivation('layers.layer_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Variable(torch.tensor(t).float().reshape(-1,1),requires_grad=True)\n",
    "y_ = model(t)\n",
    "\n",
    "\n",
    "Loss = loss(y_,torch.tensor(y).float().reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss.backward(create_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1.remove()\n",
    "h2.remove()\n",
    "h3.remove()\n",
    "\n",
    "b_h1.remove()\n",
    "b_h2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autograd\n",
    "\n",
    "first_derivative = autograd.grad(Loss, y_, create_graph=True)[0]\n",
    "second_derivative = autograd.grad(first_derivative, y_)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.]])\n"
     ]
    }
   ],
   "source": [
    "print(second_derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DNN(\n",
       "  (layers): Sequential(\n",
       "    (layer_0): Linear(in_features=1, out_features=10, bias=True)\n",
       "    (activation_0): Tanh()\n",
       "    (layer_1): Linear(in_features=10, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9666, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(2*torch.sqrt(Loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4833]], grad_fn=<BackwardHookFunctionBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4833]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forw_activation['layers.layer_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.dot(model.layers[2].weight.flatten(),forw_activation['layers.activation_0'].flatten())+ model.layers[2].bias.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_f = back_activation['layers.layer_1']*(forw_activation['layers.activation_0'])\n",
    "grad_f = (forw_activation['layers.activation_0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6889,  0.4813, -0.3163, -0.4149,  0.5590, -0.5403,  0.3087, -0.3865,\n",
       "          0.2883,  0.1710]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4746, 0.2317, 0.1001, 0.1722, 0.3125, 0.2920, 0.0953, 0.1494, 0.0831,\n",
       "        0.0293])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(grad_f*grad_f,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layers.layer_0': tensor([[ 0.8459,  0.5247, -0.3275, -0.4415,  0.6314, -0.6046,  0.3191, -0.4077,\n",
      "          0.2967,  0.1727]]), 'layers.activation_0': tensor([[ 0.6889,  0.4813, -0.3163, -0.4149,  0.5590, -0.5403,  0.3087, -0.3865,\n",
      "          0.2883,  0.1710]]), 'layers.layer_1': tensor([[-0.4833]])}\n"
     ]
    }
   ],
   "source": [
    "print(forw_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'layers.layer_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m a_1 \u001b[38;5;241m=\u001b[39m forw_activation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers.activation_0\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m g \u001b[38;5;241m=\u001b[39m back_activation[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayers.layer_1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'layers.layer_1'"
     ]
    }
   ],
   "source": [
    "a_1 = forw_activation[\"layers.activation_0\"]\n",
    "g = back_activation[\"layers.layer_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0350]])\n"
     ]
    }
   ],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2739) tensor(0.0012)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16950/1097917683.py:1: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /croot/pytorch_1686931851744/work/aten/src/ATen/native/TensorShape.cpp:3571.)\n",
      "  aa= torch.dot(a_1.reshape(-1),a_1.reshape(-1).T)\n"
     ]
    }
   ],
   "source": [
    "aa= torch.dot(a_1.reshape(-1),a_1.reshape(-1).T)\n",
    "\n",
    "gg = torch.dot(g.reshape(-1),g.reshape(-1).T)\n",
    "\n",
    "\n",
    "print(aa,gg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'layers.layer_1': tensor([[-0.0350]]),\n",
       " 'layers.activation_0': tensor([[-0.0027,  0.0046, -0.0093, -0.0023,  0.0032,  0.0069,  0.0044,  0.0065,\n",
       "           0.0086, -0.0018]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_activation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
