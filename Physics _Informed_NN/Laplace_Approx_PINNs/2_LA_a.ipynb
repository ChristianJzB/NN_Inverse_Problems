{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/s2113174/Projects-1\")\n",
    "\n",
    "#np.random.seed(1234)\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        # Number of layers\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # Activation Function\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        # The following loop organized the layers of the NN         \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1])))\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # Deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "        # for param in self.parameters():\n",
    "        #     if len(param.shape) > 1:\n",
    "        #         torch.nn.init.xavier_normal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set(max_space = 2,obs = 1,param = 1 ,mean = 0,std = 0):\n",
    "\n",
    "    t= np.linspace(0,max_space,obs)\n",
    "\n",
    "    sol = (param/ (2*np.pi))*np.sin(2*np.pi*t)\n",
    "\n",
    "    noise_sol_test = sol + np.random.normal(mean,std, len(t))\n",
    "\n",
    "    return t,noise_sol_test\n",
    "\n",
    "\n",
    "def data(max_space = 2,obs = 1,param = 1 ,mean = 0,std = 0.1):\n",
    "\n",
    "    t= np.linspace(0,max_space,obs)\n",
    "\n",
    "    sol = (param/ (2*np.pi))*np.sin(2*np.pi*t)\n",
    "\n",
    "    noise_sol_test = sol + np.random.normal(mean,std, len(t))\n",
    "\n",
    "    x,y = torch.tensor(t).float().reshape(-1,1),torch.tensor(noise_sol_test).float().reshape(-1,1)\n",
    "    \n",
    "    X_u_train = TensorDataset(x,y)\n",
    "\n",
    "    X_u_train = DataLoader(X_u_train,batch_size=obs)\n",
    "\n",
    "    return X_u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = 2\n",
    "t, y = test_set(obs = nobs)\n",
    "\n",
    "layers = [1] + 1*[10] + [1]\n",
    "model = DNN(layers)\n",
    "loss = torch.nn.MSELoss(reduction ='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.layer_0.weight\n",
      ".diag_ggn_exact.shape:    tensor([[9.6760e-07],\n",
      "        [1.8531e-02],\n",
      "        [6.1228e-03],\n",
      "        [1.4710e-04],\n",
      "        [2.2455e-01],\n",
      "        [6.6965e-02],\n",
      "        [1.6396e-01],\n",
      "        [2.0822e-04],\n",
      "        [5.3360e-04],\n",
      "        [2.2814e-01]])\n",
      "layers.layer_0.bias\n",
      ".diag_ggn_exact.shape:    tensor([0.0003, 0.0100, 0.0158, 0.0004, 0.1111, 0.0459, 0.0921, 0.0045, 0.0192,\n",
      "        0.1093])\n",
      "layers.layer_1.weight\n",
      ".diag_ggn_exact.shape:    tensor([[1.5238, 0.5709, 0.9521, 0.7752, 0.1495, 0.2799, 0.4226, 0.8920, 1.1312,\n",
      "         0.4282]])\n",
      "layers.layer_1.bias\n",
      ".diag_ggn_exact.shape:    tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "from backpack import backpack, extend\n",
    "from backpack.extensions import DiagHessian, DiagGGNExact\n",
    "\n",
    "model_ = extend(model, use_converter=True)\n",
    "lossfunc_ = extend(loss)\n",
    "\n",
    "loss_ = lossfunc_(model_(Variable(torch.tensor(t).float().reshape(-1,1),requires_grad=True)), torch.tensor(y).float().reshape(-1,1))\n",
    "\n",
    "with backpack(DiagHessian(), DiagGGNExact()):\n",
    "    loss_.backward()\n",
    "\n",
    "for name, param in model_.named_parameters():\n",
    "    print(name)\n",
    "    print(\".diag_ggn_exact.shape:   \", param.diag_ggn_exact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5238, 0.5709, 0.9521, 0.7752, 0.1495, 0.2799, 0.4226, 0.8920, 1.1312,\n",
      "        0.4282, 2.0000])\n",
      "tensor([[[1.3187]],\n",
      "\n",
      "        [[3.1874]]])\n"
     ]
    }
   ],
   "source": [
    "from laplace import Laplace\n",
    "\n",
    "la = Laplace(model, 'regression', subset_of_weights='last_layer', hessian_structure='diag')\n",
    "\n",
    "dta = data(obs = nobs)\n",
    "\n",
    "la.fit(dta)\n",
    "\n",
    "print(la.H)\n",
    "\n",
    "x,y = next(iter(dta))\n",
    "\n",
    "#la.model.forward_with_features(x)\n",
    "\n",
    "fm, varl = la(x)\n",
    "\n",
    "print(varl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict to store the activations\n",
    "forw_activation = {}\n",
    "def forw_getActivation(name):\n",
    "  # the hook signature\n",
    "  def hook(model, input, output):\n",
    "    forw_activation[name] = output.detach()\n",
    "  return hook\n",
    "\n",
    "h1 = model.layers[1].register_forward_hook(forw_getActivation('layers.activation_0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13844/4003631245.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Loss = loss(y_,torch.tensor(y).float().reshape(-1,1))\n"
     ]
    }
   ],
   "source": [
    "t = Variable(torch.tensor(t).float().reshape(-1,1),requires_grad=True)\n",
    "y_ = model(t)\n",
    "\n",
    "h1.remove()\n",
    "\n",
    "Loss = loss(y_,torch.tensor(y).float().reshape(-1,1))\n",
    "\n",
    "df_f = grad(Loss, y_, create_graph=True)[0]\n",
    "\n",
    "ddf_ff = grad(df_f, y_, torch.ones_like(df_f))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt,bias = model.layers[-1].weight, model.layers[-1].bias\n",
    "\n",
    "param_MAP = torch.cat((wt,bias.reshape(1,1)),1) \n",
    "\n",
    "nparam = param_MAP.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5238, 0.5709, 0.9521, 0.7752, 0.1495, 0.2799, 0.4226, 0.8920, 1.1312,\n",
      "        0.4282, 2.0000])\n",
      "2.8429605e-07\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "df_theta = torch.cat((forw_activation['layers.activation_0'],torch.ones_like(ddf_ff)),1)\n",
    "\n",
    "H = (nobs/2)*torch.sum(df_theta*ddf_ff*df_theta,axis=0)\n",
    "\n",
    "print(H)\n",
    "\n",
    "print(np.linalg.norm((H-la.H),ord=2))\n",
    "print(np.allclose(H,la.H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, phi = y_,forw_activation['layers.activation_0']\n",
    "\n",
    "bsize = phi.shape[0]\n",
    "output_size = f.shape[-1]\n",
    "\n",
    "# calculate Jacobians using the feature vector 'phi'\n",
    "identity = torch.eye(output_size, device=x.device).unsqueeze(0).tile(bsize, 1, 1)\n",
    "# Jacobians are batch x output x params\n",
    "Js = torch.einsum('kp,kij->kijp', phi, identity).reshape(bsize, output_size, -1)\n",
    "Js = torch.cat([Js, identity], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_noise():\n",
    "    return _sigma_noise\n",
    "\n",
    "def _H_factor():\n",
    "    sigma2 = sigma_noise().square()\n",
    "    return 1 / sigma2 / temperature\n",
    "\n",
    "def prior_precision_diag(prior_precision,n_params):\n",
    "    \"\"\"Obtain the diagonal prior precision \\\\(p_0\\\\) constructed from either\n",
    "    a scalar, layer-wise, or diagonal prior precision.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prior_precision_diag : torch.Tensor\n",
    "    \"\"\"\n",
    "    if len(prior_precision) == 1:  # scalar\n",
    "        return prior_precision * torch.ones(n_params, device=device)\n",
    "\n",
    "    elif len(prior_precision) == n_params:  # diagonal\n",
    "        return prior_precision\n",
    "\n",
    "    # elif len(prior_precision) == n_layers:  # per layer\n",
    "    #     n_params_per_layer = parameters_per_layer(self.model)\n",
    "    #     return torch.cat([prior * torch.ones(n_params, device=self._device) for prior, n_params\n",
    "    #                         in zip(self.prior_precision, n_params_per_layer)])\n",
    "\n",
    "    # else:\n",
    "    #     raise ValueError('Mismatch of prior and model. Diagonal, scalar, or per-layer prior.')\n",
    "\n",
    "_sigma_noise=torch.tensor([1])\n",
    "temperature=torch.tensor([1])\n",
    "prior_precision=torch.tensor([1])\n",
    "\n",
    "prior_precision_diag = prior_precision_diag(prior_precision,nparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6785, 2.7301])\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "post_presicion = _H_factor() * H + prior_precision_diag\n",
    "\n",
    "\n",
    "post_variance = 1 / post_presicion\n",
    "\n",
    "\n",
    "functional_var = torch.einsum('ncp,p,nkp->nck', Js, post_variance, Js)\n",
    "\n",
    "print(functional_var.flatten())\n",
    "\n",
    "print(np.linalg.norm((functional_var.flatten()-varl.flatten()),ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last-layer Full Hessian \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2130, -0.7153, -0.8653, -0.4236,  0.4071,  0.9955, -0.3935, -0.6093,\n",
      "         -1.3308, -1.0323,  1.4968],\n",
      "        [-0.7153,  0.4655,  0.5691,  0.4405, -0.4495, -0.6220,  0.1000,  0.5000,\n",
      "          0.7534,  0.6137, -0.8008],\n",
      "        [-0.8653,  0.5691,  0.6966,  0.5589, -0.5724, -0.7573,  0.1029,  0.6242,\n",
      "          0.9073,  0.7432, -0.9576],\n",
      "        [-0.4236,  0.4405,  0.5589,  0.9796, -1.0555, -0.5002, -0.4386,  0.8266,\n",
      "          0.3284,  0.3825, -0.1659],\n",
      "        [ 0.4071, -0.4495, -0.5724, -1.0555,  1.1397,  0.5017,  0.5005, -0.8786,\n",
      "         -0.2970, -0.3707,  0.1105],\n",
      "        [ 0.9955, -0.6220, -0.7573, -0.5002,  0.5017,  0.8450, -0.2172, -0.6127,\n",
      "         -1.0671, -0.8512,  1.1629],\n",
      "        [-0.3935,  0.1000,  0.1029, -0.4386,  0.5005, -0.2172,  0.5265, -0.2274,\n",
      "          0.5261,  0.3196, -0.7326],\n",
      "        [-0.6093,  0.5000,  0.6242,  0.8266, -0.8786, -0.6127, -0.2274,  0.7591,\n",
      "          0.5679,  0.5348, -0.4885],\n",
      "        [-1.3308,  0.7534,  0.9073,  0.3284, -0.2970, -1.0671,  0.5261,  0.5679,\n",
      "          1.4823,  1.1289, -1.7005],\n",
      "        [-1.0323,  0.6137,  0.7432,  0.3825, -0.3707, -0.8512,  0.3196,  0.5348,\n",
      "          1.1289,  0.8791, -1.2643],\n",
      "        [ 1.4968, -0.8008, -0.9576, -0.1659,  0.1105,  1.1629, -0.7326, -0.4885,\n",
      "         -1.7005, -1.2643,  2.0000]])\n",
      "tensor([[[0.7639]],\n",
      "\n",
      "        [[0.8708]]])\n"
     ]
    }
   ],
   "source": [
    "model = DNN(layers)\n",
    "\n",
    "la1 = Laplace(model, 'regression', subset_of_weights='last_layer', hessian_structure='full')\n",
    "\n",
    "dta = data(obs = nobs)\n",
    "\n",
    "la1.fit(dta)\n",
    "\n",
    "print(la1.H)\n",
    "\n",
    "x,y = next(iter(dta))\n",
    "\n",
    "#la.model.forward_with_features(x)\n",
    "\n",
    "fm, varl = la1(x)\n",
    "\n",
    "print(varl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2130, -0.7153, -0.8653, -0.4236,  0.4071,  0.9955, -0.3935, -0.6093,\n",
      "         -1.3308, -1.0323,  1.4968],\n",
      "        [-0.7153,  0.4655,  0.5691,  0.4405, -0.4495, -0.6220,  0.1000,  0.5000,\n",
      "          0.7534,  0.6137, -0.8008],\n",
      "        [-0.8653,  0.5691,  0.6966,  0.5589, -0.5724, -0.7573,  0.1029,  0.6242,\n",
      "          0.9073,  0.7432, -0.9576],\n",
      "        [-0.4236,  0.4405,  0.5589,  0.9796, -1.0555, -0.5002, -0.4386,  0.8266,\n",
      "          0.3284,  0.3825, -0.1659],\n",
      "        [ 0.4071, -0.4495, -0.5724, -1.0555,  1.1397,  0.5017,  0.5005, -0.8786,\n",
      "         -0.2970, -0.3707,  0.1105],\n",
      "        [ 0.9955, -0.6220, -0.7573, -0.5002,  0.5017,  0.8450, -0.2172, -0.6127,\n",
      "         -1.0671, -0.8512,  1.1629],\n",
      "        [-0.3935,  0.1000,  0.1029, -0.4386,  0.5005, -0.2172,  0.5265, -0.2274,\n",
      "          0.5261,  0.3196, -0.7326],\n",
      "        [-0.6093,  0.5000,  0.6242,  0.8266, -0.8786, -0.6127, -0.2274,  0.7591,\n",
      "          0.5679,  0.5348, -0.4885],\n",
      "        [-1.3308,  0.7534,  0.9073,  0.3284, -0.2970, -1.0671,  0.5261,  0.5679,\n",
      "          1.4823,  1.1289, -1.7005],\n",
      "        [-1.0323,  0.6137,  0.7432,  0.3825, -0.3707, -0.8512,  0.3196,  0.5348,\n",
      "          1.1289,  0.8791, -1.2643],\n",
      "        [ 1.4968, -0.8008, -0.9576, -0.1659,  0.1105,  1.1629, -0.7326, -0.4885,\n",
      "         -1.7005, -1.2643,  2.0000]])\n",
      "0.0\n",
      "tensor([0.7639, 0.8708])\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13844/3255267061.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = Variable(torch.tensor(t).float().reshape(-1,1),requires_grad=True)\n",
      "/tmp/ipykernel_13844/3255267061.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Loss = loss(y_,torch.tensor(y).float().reshape(-1,1))\n"
     ]
    }
   ],
   "source": [
    "from torch.distributions.multivariate_normal import _precision_to_scale_tril\n",
    "\n",
    "\n",
    "# a dict to store the activations\n",
    "forw_activation = {}\n",
    "def forw_getActivation(name):\n",
    "  # the hook signature\n",
    "  def hook(model, input, output):\n",
    "    forw_activation[name] = output.detach()\n",
    "  return hook\n",
    "\n",
    "h1 = model.layers[1].register_forward_hook(forw_getActivation('layers.activation_0'))\n",
    "\n",
    "t = Variable(torch.tensor(t).float().reshape(-1,1),requires_grad=True)\n",
    "y_ = model(t)\n",
    "\n",
    "h1.remove()\n",
    "\n",
    "Loss = loss(y_,torch.tensor(y).float().reshape(-1,1))\n",
    "\n",
    "df_f = grad(Loss, y_, create_graph=True)[0]\n",
    "\n",
    "ddf_ff = grad(df_f, y_, torch.ones_like(df_f))[0]\n",
    "\n",
    "wt,bias = model.layers[-1].weight, model.layers[-1].bias\n",
    "\n",
    "param_MAP = torch.cat((wt,bias.reshape(1,1)),1) \n",
    "\n",
    "nparam = param_MAP.reshape(-1).shape\n",
    "\n",
    "df_theta = torch.cat((forw_activation['layers.activation_0'],torch.ones_like(ddf_ff)),1)\n",
    "\n",
    "H = (nobs/2)*torch.sum(torch.einsum('bc,bd->bcd', df_theta, df_theta),axis=0)\n",
    "\n",
    "print(H)\n",
    "\n",
    "print(np.linalg.norm((H-la1.H),ord=2))\n",
    "\n",
    "\n",
    "f, phi = y_,forw_activation['layers.activation_0']\n",
    "\n",
    "bsize = phi.shape[0]\n",
    "output_size = f.shape[-1]\n",
    "\n",
    "# calculate Jacobians using the feature vector 'phi'\n",
    "identity = torch.eye(output_size, device=x.device).unsqueeze(0).tile(bsize, 1, 1)\n",
    "# Jacobians are batch x output x params\n",
    "Js = torch.einsum('kp,kij->kijp', phi, identity).reshape(bsize, output_size, -1)\n",
    "Js = torch.cat([Js, identity], dim=2)\n",
    "\n",
    "\n",
    "\n",
    "post_presicion = _H_factor() * H + torch.diag(prior_precision_diag)\n",
    "\n",
    "post_scale = _precision_to_scale_tril(post_presicion)\n",
    "\n",
    "post_cov = post_scale @ post_scale.T\n",
    "\n",
    "functional_var = torch.einsum('ncp,pq,nkq->nck', Js, post_cov, Js)\n",
    "\n",
    "print(functional_var.flatten())\n",
    "\n",
    "print(np.linalg.norm((functional_var.flatten()-varl.flatten()),ord=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
