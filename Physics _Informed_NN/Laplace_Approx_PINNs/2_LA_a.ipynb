{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable, grad\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(\"/home/s2113174/Projects-1\")\n",
    "\n",
    "#np.random.seed(1234)\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep neural network\n",
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        # Number of layers\n",
    "        self.depth = len(layers) - 1\n",
    "        \n",
    "        # Activation Function\n",
    "        self.activation = torch.nn.Tanh\n",
    "        \n",
    "        # The following loop organized the layers of the NN         \n",
    "        layer_list = list()\n",
    "        for i in range(self.depth - 1): \n",
    "            layer_list.append(\n",
    "                ('layer_%d' % i, torch.nn.Linear(layers[i], layers[i+1])))\n",
    "            layer_list.append(('activation_%d' % i, self.activation()))\n",
    "        layer_list.append(\n",
    "            ('layer_%d' % (self.depth - 1), torch.nn.Linear(layers[-2], layers[-1]))\n",
    "        )\n",
    "        layerDict = OrderedDict(layer_list)\n",
    "        \n",
    "        # Deploy layers\n",
    "        self.layers = torch.nn.Sequential(layerDict)\n",
    "\n",
    "        # for param in self.parameters():\n",
    "        #     if len(param.shape) > 1:\n",
    "        #         torch.nn.init.xavier_normal_(param)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_set(max_space = 2,obs = 1,param = 1 ,mean = 0,std = 0):\n",
    "\n",
    "    t= np.linspace(0,max_space,obs)\n",
    "\n",
    "    sol = (param/ (2*np.pi))*np.sin(2*np.pi*t)\n",
    "\n",
    "    noise_sol_test = sol + np.random.normal(mean,std, len(t))\n",
    "\n",
    "    return t,noise_sol_test\n",
    "\n",
    "\n",
    "def data(max_space = 2,obs = 1,param = 1 ,mean = 0,std = 0.1):\n",
    "\n",
    "    t= np.linspace(0,max_space,obs)\n",
    "\n",
    "    sol = (param/ (2*np.pi))*np.sin(2*np.pi*t)\n",
    "\n",
    "    noise_sol_test = sol + np.random.normal(mean,std, len(t))\n",
    "\n",
    "    x,y = torch.tensor(t).float().reshape(-1,1),torch.tensor(noise_sol_test).float().reshape(-1,1)\n",
    "    \n",
    "    X_u_train = TensorDataset(x,y)\n",
    "\n",
    "    X_u_train = DataLoader(X_u_train,batch_size=obs)\n",
    "\n",
    "    return X_u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobs = 100\n",
    "t, y = test_set(obs = nobs)\n",
    "\n",
    "layers = [1] + 1*[10] + [1]\n",
    "model = DNN(layers)\n",
    "loss = torch.nn.MSELoss(reduction ='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.layer_0.weight\n",
      ".diag_ggn_exact.shape:    tensor([[7.4235e-05],\n",
      "        [2.6942e-02],\n",
      "        [1.2357e-03],\n",
      "        [6.4169e-02],\n",
      "        [2.6704e-03],\n",
      "        [7.5335e-05],\n",
      "        [1.0758e-02],\n",
      "        [1.4902e-02],\n",
      "        [7.6811e-02],\n",
      "        [1.4195e-02]])\n",
      "layers.layer_0.bias\n",
      ".diag_ggn_exact.shape:    tensor([0.0002, 0.0211, 0.0013, 0.0479, 0.0090, 0.0002, 0.0116, 0.0116, 0.0718,\n",
      "        0.0253])\n",
      "layers.layer_1.weight\n",
      ".diag_ggn_exact.shape:    tensor([[0.9941, 0.7067, 1.2323, 0.0028, 1.5282, 1.1405, 0.9663, 0.4120, 0.2626,\n",
      "         0.6643]])\n",
      "layers.layer_1.bias\n",
      ".diag_ggn_exact.shape:    tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "from backpack import backpack, extend\n",
    "from backpack.extensions import DiagHessian, DiagGGNExact\n",
    "\n",
    "model_ = extend(model, use_converter=True)\n",
    "lossfunc_ = extend(loss)\n",
    "\n",
    "loss_ = lossfunc_(model_(Variable(torch.tensor(t).float().reshape(-1,1),requires_grad=True)), torch.tensor(y).float().reshape(-1,1))\n",
    "\n",
    "with backpack(DiagHessian(), DiagGGNExact()):\n",
    "    loss_.backward()\n",
    "\n",
    "for name, param in model_.named_parameters():\n",
    "    print(name)\n",
    "    print(\".diag_ggn_exact.shape:   \", param.diag_ggn_exact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 49.7045,  35.3336,  61.6160,   0.1379,  76.4101,  57.0230,  48.3130,\n",
      "         20.6006,  13.1279,  33.2144, 100.0000])\n",
      "tensor([[[0.0842]],\n",
      "\n",
      "        [[0.0830]],\n",
      "\n",
      "        [[0.0819]],\n",
      "\n",
      "        [[0.0808]],\n",
      "\n",
      "        [[0.0797]],\n",
      "\n",
      "        [[0.0787]],\n",
      "\n",
      "        [[0.0777]],\n",
      "\n",
      "        [[0.0767]],\n",
      "\n",
      "        [[0.0758]],\n",
      "\n",
      "        [[0.0750]],\n",
      "\n",
      "        [[0.0742]],\n",
      "\n",
      "        [[0.0734]],\n",
      "\n",
      "        [[0.0727]],\n",
      "\n",
      "        [[0.0720]],\n",
      "\n",
      "        [[0.0714]],\n",
      "\n",
      "        [[0.0708]],\n",
      "\n",
      "        [[0.0702]],\n",
      "\n",
      "        [[0.0697]],\n",
      "\n",
      "        [[0.0693]],\n",
      "\n",
      "        [[0.0689]],\n",
      "\n",
      "        [[0.0686]],\n",
      "\n",
      "        [[0.0683]],\n",
      "\n",
      "        [[0.0680]],\n",
      "\n",
      "        [[0.0678]],\n",
      "\n",
      "        [[0.0677]],\n",
      "\n",
      "        [[0.0676]],\n",
      "\n",
      "        [[0.0676]],\n",
      "\n",
      "        [[0.0676]],\n",
      "\n",
      "        [[0.0677]],\n",
      "\n",
      "        [[0.0678]],\n",
      "\n",
      "        [[0.0680]],\n",
      "\n",
      "        [[0.0682]],\n",
      "\n",
      "        [[0.0685]],\n",
      "\n",
      "        [[0.0688]],\n",
      "\n",
      "        [[0.0692]],\n",
      "\n",
      "        [[0.0696]],\n",
      "\n",
      "        [[0.0701]],\n",
      "\n",
      "        [[0.0706]],\n",
      "\n",
      "        [[0.0712]],\n",
      "\n",
      "        [[0.0719]],\n",
      "\n",
      "        [[0.0726]],\n",
      "\n",
      "        [[0.0733]],\n",
      "\n",
      "        [[0.0741]],\n",
      "\n",
      "        [[0.0750]],\n",
      "\n",
      "        [[0.0759]],\n",
      "\n",
      "        [[0.0768]],\n",
      "\n",
      "        [[0.0778]],\n",
      "\n",
      "        [[0.0789]],\n",
      "\n",
      "        [[0.0800]],\n",
      "\n",
      "        [[0.0811]],\n",
      "\n",
      "        [[0.0823]],\n",
      "\n",
      "        [[0.0836]],\n",
      "\n",
      "        [[0.0849]],\n",
      "\n",
      "        [[0.0862]],\n",
      "\n",
      "        [[0.0876]],\n",
      "\n",
      "        [[0.0890]],\n",
      "\n",
      "        [[0.0904]],\n",
      "\n",
      "        [[0.0919]],\n",
      "\n",
      "        [[0.0935]],\n",
      "\n",
      "        [[0.0950]],\n",
      "\n",
      "        [[0.0966]],\n",
      "\n",
      "        [[0.0983]],\n",
      "\n",
      "        [[0.0999]],\n",
      "\n",
      "        [[0.1016]],\n",
      "\n",
      "        [[0.1034]],\n",
      "\n",
      "        [[0.1051]],\n",
      "\n",
      "        [[0.1069]],\n",
      "\n",
      "        [[0.1087]],\n",
      "\n",
      "        [[0.1106]],\n",
      "\n",
      "        [[0.1124]],\n",
      "\n",
      "        [[0.1143]],\n",
      "\n",
      "        [[0.1162]],\n",
      "\n",
      "        [[0.1181]],\n",
      "\n",
      "        [[0.1200]],\n",
      "\n",
      "        [[0.1220]],\n",
      "\n",
      "        [[0.1239]],\n",
      "\n",
      "        [[0.1259]],\n",
      "\n",
      "        [[0.1278]],\n",
      "\n",
      "        [[0.1298]],\n",
      "\n",
      "        [[0.1318]],\n",
      "\n",
      "        [[0.1338]],\n",
      "\n",
      "        [[0.1358]],\n",
      "\n",
      "        [[0.1377]],\n",
      "\n",
      "        [[0.1397]],\n",
      "\n",
      "        [[0.1417]],\n",
      "\n",
      "        [[0.1437]],\n",
      "\n",
      "        [[0.1457]],\n",
      "\n",
      "        [[0.1476]],\n",
      "\n",
      "        [[0.1496]],\n",
      "\n",
      "        [[0.1516]],\n",
      "\n",
      "        [[0.1535]],\n",
      "\n",
      "        [[0.1554]],\n",
      "\n",
      "        [[0.1574]],\n",
      "\n",
      "        [[0.1593]],\n",
      "\n",
      "        [[0.1612]],\n",
      "\n",
      "        [[0.1631]],\n",
      "\n",
      "        [[0.1649]],\n",
      "\n",
      "        [[0.1668]],\n",
      "\n",
      "        [[0.1686]],\n",
      "\n",
      "        [[0.1704]]])\n"
     ]
    }
   ],
   "source": [
    "from laplace import Laplace\n",
    "\n",
    "la = Laplace(model, 'regression', subset_of_weights='last_layer', hessian_structure='diag')\n",
    "\n",
    "dta = data(obs = nobs)\n",
    "\n",
    "la.fit(dta)\n",
    "\n",
    "print(la.H)\n",
    "\n",
    "x,y = next(iter(dta))\n",
    "\n",
    "#la.model.forward_with_features(x)\n",
    "\n",
    "fm, varl = la(x)\n",
    "\n",
    "print(varl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict to store the activations\n",
    "forw_activation = {}\n",
    "def forw_getActivation(name):\n",
    "  # the hook signature\n",
    "  def hook(model, input, output):\n",
    "    forw_activation[name] = output.detach()\n",
    "  return hook\n",
    "\n",
    "h1 = model.layers[1].register_forward_hook(forw_getActivation('layers.activation_0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22411/4003631245.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Loss = loss(y_,torch.tensor(y).float().reshape(-1,1))\n"
     ]
    }
   ],
   "source": [
    "t = Variable(torch.tensor(t).float().reshape(-1,1),requires_grad=True)\n",
    "y_ = model(t)\n",
    "\n",
    "h1.remove()\n",
    "\n",
    "Loss = loss(y_,torch.tensor(y).float().reshape(-1,1))\n",
    "\n",
    "df_f = grad(Loss, y_, create_graph=True)[0]\n",
    "\n",
    "ddf_ff = grad(df_f, y_, torch.ones_like(df_f))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt,bias = model.layers[-1].weight, model.layers[-1].bias\n",
    "\n",
    "param_MAP = torch.cat((wt,bias.reshape(1,1)),1) \n",
    "\n",
    "nparam = param_MAP.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 49.7045,  35.3336,  61.6160,   0.1379,  76.4101,  57.0230,  48.3130,\n",
      "         20.6006,  13.1279,  33.2144, 100.0000])\n"
     ]
    }
   ],
   "source": [
    "df_theta = torch.cat((forw_activation['layers.activation_0'],torch.ones_like(ddf_ff)),1)\n",
    "\n",
    "H = (nobs/2)*torch.sum(df_theta*ddf_ff*df_theta,axis=0)\n",
    "\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, phi = y_,forw_activation['layers.activation_0']\n",
    "\n",
    "bsize = phi.shape[0]\n",
    "output_size = f.shape[-1]\n",
    "\n",
    "# calculate Jacobians using the feature vector 'phi'\n",
    "identity = torch.eye(output_size, device=x.device).unsqueeze(0).tile(bsize, 1, 1)\n",
    "# Jacobians are batch x output x params\n",
    "Js = torch.einsum('kp,kij->kijp', phi, identity).reshape(bsize, output_size, -1)\n",
    "Js = torch.cat([Js, identity], dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_noise():\n",
    "    return _sigma_noise\n",
    "\n",
    "def _H_factor():\n",
    "    sigma2 = sigma_noise().square()\n",
    "    return 1 / sigma2 / temperature\n",
    "\n",
    "def prior_precision_diag(prior_precision,n_params):\n",
    "    \"\"\"Obtain the diagonal prior precision \\\\(p_0\\\\) constructed from either\n",
    "    a scalar, layer-wise, or diagonal prior precision.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prior_precision_diag : torch.Tensor\n",
    "    \"\"\"\n",
    "    if len(prior_precision) == 1:  # scalar\n",
    "        return prior_precision * torch.ones(n_params, device=device)\n",
    "\n",
    "    elif len(prior_precision) == n_params:  # diagonal\n",
    "        return prior_precision\n",
    "\n",
    "    # elif len(prior_precision) == n_layers:  # per layer\n",
    "    #     n_params_per_layer = parameters_per_layer(self.model)\n",
    "    #     return torch.cat([prior * torch.ones(n_params, device=self._device) for prior, n_params\n",
    "    #                         in zip(self.prior_precision, n_params_per_layer)])\n",
    "\n",
    "    # else:\n",
    "    #     raise ValueError('Mismatch of prior and model. Diagonal, scalar, or per-layer prior.')\n",
    "\n",
    "_sigma_noise=torch.tensor([1])\n",
    "temperature=torch.tensor([1])\n",
    "prior_precision=torch.tensor([1])\n",
    "\n",
    "prior_precision_diag = prior_precision_diag(prior_precision,nparam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0842]],\n",
      "\n",
      "        [[0.0830]],\n",
      "\n",
      "        [[0.0819]],\n",
      "\n",
      "        [[0.0808]],\n",
      "\n",
      "        [[0.0797]],\n",
      "\n",
      "        [[0.0787]],\n",
      "\n",
      "        [[0.0777]],\n",
      "\n",
      "        [[0.0767]],\n",
      "\n",
      "        [[0.0758]],\n",
      "\n",
      "        [[0.0750]],\n",
      "\n",
      "        [[0.0742]],\n",
      "\n",
      "        [[0.0734]],\n",
      "\n",
      "        [[0.0727]],\n",
      "\n",
      "        [[0.0720]],\n",
      "\n",
      "        [[0.0714]],\n",
      "\n",
      "        [[0.0708]],\n",
      "\n",
      "        [[0.0702]],\n",
      "\n",
      "        [[0.0697]],\n",
      "\n",
      "        [[0.0693]],\n",
      "\n",
      "        [[0.0689]],\n",
      "\n",
      "        [[0.0686]],\n",
      "\n",
      "        [[0.0683]],\n",
      "\n",
      "        [[0.0680]],\n",
      "\n",
      "        [[0.0678]],\n",
      "\n",
      "        [[0.0677]],\n",
      "\n",
      "        [[0.0676]],\n",
      "\n",
      "        [[0.0676]],\n",
      "\n",
      "        [[0.0676]],\n",
      "\n",
      "        [[0.0677]],\n",
      "\n",
      "        [[0.0678]],\n",
      "\n",
      "        [[0.0680]],\n",
      "\n",
      "        [[0.0682]],\n",
      "\n",
      "        [[0.0685]],\n",
      "\n",
      "        [[0.0688]],\n",
      "\n",
      "        [[0.0692]],\n",
      "\n",
      "        [[0.0696]],\n",
      "\n",
      "        [[0.0701]],\n",
      "\n",
      "        [[0.0706]],\n",
      "\n",
      "        [[0.0712]],\n",
      "\n",
      "        [[0.0719]],\n",
      "\n",
      "        [[0.0726]],\n",
      "\n",
      "        [[0.0733]],\n",
      "\n",
      "        [[0.0741]],\n",
      "\n",
      "        [[0.0750]],\n",
      "\n",
      "        [[0.0759]],\n",
      "\n",
      "        [[0.0768]],\n",
      "\n",
      "        [[0.0778]],\n",
      "\n",
      "        [[0.0789]],\n",
      "\n",
      "        [[0.0800]],\n",
      "\n",
      "        [[0.0811]],\n",
      "\n",
      "        [[0.0823]],\n",
      "\n",
      "        [[0.0836]],\n",
      "\n",
      "        [[0.0849]],\n",
      "\n",
      "        [[0.0862]],\n",
      "\n",
      "        [[0.0876]],\n",
      "\n",
      "        [[0.0890]],\n",
      "\n",
      "        [[0.0904]],\n",
      "\n",
      "        [[0.0919]],\n",
      "\n",
      "        [[0.0935]],\n",
      "\n",
      "        [[0.0950]],\n",
      "\n",
      "        [[0.0966]],\n",
      "\n",
      "        [[0.0983]],\n",
      "\n",
      "        [[0.0999]],\n",
      "\n",
      "        [[0.1016]],\n",
      "\n",
      "        [[0.1034]],\n",
      "\n",
      "        [[0.1051]],\n",
      "\n",
      "        [[0.1069]],\n",
      "\n",
      "        [[0.1087]],\n",
      "\n",
      "        [[0.1106]],\n",
      "\n",
      "        [[0.1124]],\n",
      "\n",
      "        [[0.1143]],\n",
      "\n",
      "        [[0.1162]],\n",
      "\n",
      "        [[0.1181]],\n",
      "\n",
      "        [[0.1200]],\n",
      "\n",
      "        [[0.1220]],\n",
      "\n",
      "        [[0.1239]],\n",
      "\n",
      "        [[0.1259]],\n",
      "\n",
      "        [[0.1278]],\n",
      "\n",
      "        [[0.1298]],\n",
      "\n",
      "        [[0.1318]],\n",
      "\n",
      "        [[0.1338]],\n",
      "\n",
      "        [[0.1358]],\n",
      "\n",
      "        [[0.1377]],\n",
      "\n",
      "        [[0.1397]],\n",
      "\n",
      "        [[0.1417]],\n",
      "\n",
      "        [[0.1437]],\n",
      "\n",
      "        [[0.1457]],\n",
      "\n",
      "        [[0.1476]],\n",
      "\n",
      "        [[0.1496]],\n",
      "\n",
      "        [[0.1516]],\n",
      "\n",
      "        [[0.1535]],\n",
      "\n",
      "        [[0.1554]],\n",
      "\n",
      "        [[0.1574]],\n",
      "\n",
      "        [[0.1593]],\n",
      "\n",
      "        [[0.1612]],\n",
      "\n",
      "        [[0.1631]],\n",
      "\n",
      "        [[0.1649]],\n",
      "\n",
      "        [[0.1668]],\n",
      "\n",
      "        [[0.1686]],\n",
      "\n",
      "        [[0.1704]]])\n"
     ]
    }
   ],
   "source": [
    "post_presicion = _H_factor() * H + prior_precision_diag\n",
    "\n",
    "\n",
    "post_variance = 1 / post_presicion\n",
    "\n",
    "\n",
    "functional_var = torch.einsum('ncp,p,nkp->nck', Js, post_variance, Js)\n",
    "\n",
    "print(functional_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
